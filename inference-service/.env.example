# ServiceB（推論サービス）環境変数設定

# ポート設定
PORT=8080

# ログレベル設定
LOG_LEVEL=INFO

# ONNX Runtime設定
ORT_INTRA_THREADS=4
ORT_INTER_THREADS=2

# CTranslate2設定
CT2_INTER_THREADS=1
CT2_INTRA_THREADS=4

# バッチ処理設定
# 並列ワーカー数（デフォルト: CPU数）
# CPU2コア環境では2を推奨
BATCH_PARALLEL_WORKERS=2

# レート制限設定
RATE_LIMIT_LAYOUT=60/minute
RATE_LIMIT_LAYOUT_BATCH=20/minute
RATE_LIMIT_TRANSLATE=300/minute

# モデルパス
LAYOUT_MODEL_PATH=models/layout_m.onnx
LOCAL_MODEL_PATH=models/m2m100_ct2
# Llama-cpp (LLM) 設定
LLAMACPP_REPO_ID=byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF
LLAMACPP_FILENAME=Q3_K_S-2.66bow
# LLAMACPP_MODEL_PATH=models/qwen.gguf # ローカルファイルを使用する場合
LLAMACPP_CTX_SIZE=4096
LLAMACPP_THREADS=4
LLAMACPP_GPU_LAYERS=0

# Llama-cpp (LLM) 最適化設定 (Intel i7-9750H用)
LLAMACPP_THREADS=4
LLAMACPP_BATCH_SIZE=512
